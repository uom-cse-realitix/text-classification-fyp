{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVB7GyPmE0Fm"
   },
   "source": [
    "## NOTE:\n",
    "\n",
    "Create a folder named `data` in the files tab and upload the dataset files from https://github.com/uom-cse-realitix/text-classification-fyp/tree/master/data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRu_bwnmEWnL"
   },
   "source": [
    "## NOTE:\n",
    "**Tensorflow and Keras versions should same as that in local machine or lstm model won't work.**\n",
    "Check local versions and use the below cells to change Colab versions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2UDlKqN8AFK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgK0023c6eeL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUR-9Ewh6fMy"
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 32\n",
    "# Stop words\n",
    "stopwords_list = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "                  \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\",\n",
    "                  \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"which\", \"who\", \"whom\", \"these\",\n",
    "                  \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\",\n",
    "                  \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "                  \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"against\", \"into\", \"through\", \"during\",\n",
    "                  \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\",\n",
    "                  \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"why\", \"how\", \"all\", \"any\",\n",
    "                  \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\",\n",
    "                  \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "\n",
    "def import_and_prepare(filepath):\n",
    "    df = pd.read_csv(filepath, names=['sentence', 'operation'], sep=',', engine='python')\n",
    "    # df = shuffle(df)\n",
    "    sentences = df['sentence'].values\n",
    "    y = df['operation'].values\n",
    "    return df, sentences, y\n",
    "\n",
    "\n",
    "def filter_stopwords(sentences, stopwords_list):\n",
    "    stopwords_set = set(stopwords_list)\n",
    "    filtered = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = word_tokenize(sentence)\n",
    "        filtered_sentence = []\n",
    "        for w in tokenized_sentence:\n",
    "            if w not in stopwords_set:\n",
    "                filtered_sentence.append(w)\n",
    "        filtered.append(filtered_sentence)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def detokenize(filtered_sentences):\n",
    "    detokenized_sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        detokenized_sentences.append(TreebankWordDetokenizer().detokenize(sentence))\n",
    "    return detokenized_sentences\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_label_distribution(dataframe):\n",
    "    dataframe['operation'].value_counts().plot(kind=\"bar\")\n",
    "\n",
    "\n",
    "def init_tokenizer(MAX_NB_WORDS, dataframe):\n",
    "    tokenizer = Tokenizer(MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(dataframe['filtered_sentence'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_model(max_words, embedding_dimensions, X):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dimensions, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def lstm_train(df, tokenizer, max_sequence_length, embedding_dimensions):\n",
    "    X = tokenizer.texts_to_sequences(df['filtered_sentence'].values)\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    print('Shape of data tensor:', X.shape)\n",
    "    Y = pd.get_dummies(df['operation']).values\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "\n",
    "    # Oversampling the minority class\n",
    "    smote = SMOTE('minority')\n",
    "    X_train, Y_train = smote.fit_sample(X_train, Y_train)\n",
    "\n",
    "    model = create_model(max_sequence_length, embedding_dimensions, X)\n",
    "    epochs = 150\n",
    "    batch_size = 100\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        validation_split=0.1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "    accr = model.evaluate(X_test, Y_test)\n",
    "    print(model.summary())\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))\n",
    "    # plot_model(model, to_file='model.png')\n",
    "    return model, history\n",
    "\n",
    "def infer(sentence, tokenizer, model):\n",
    "    sentence_as_array = [sentence]\n",
    "    filtered_commands = filter_stopwords(sentence_as_array, stopwords_list)\n",
    "    seq = tokenizer.texts_to_sequences(filtered_commands)\n",
    "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    pred = model.predict(padded)\n",
    "    return pred\n",
    "\n",
    "def pre_initialize():\n",
    "    df, sentences, y = import_and_prepare('data/dataset_new.txt')\n",
    "    # df_temp, sentences_temp, y_temp = import_and_prepare('data/dataset_new.txt')\n",
    "    plot_label_distribution(df)\n",
    "    filtered_sentences = filter_stopwords(sentences, stopwords_list)\n",
    "    detokenized_sentences = detokenize(filtered_sentences)\n",
    "    df['filtered_sentence'] = detokenized_sentences\n",
    "    tokenizer = init_tokenizer(MAX_NB_WORDS, df)\n",
    "    return df, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 unique tokens.\n",
      "Shape of data tensor: (709, 250)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, tokenizer = pre_initialize()\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['filtered_sentence'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "Y = df['operation'].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dictionary = {\n",
    "    64: {\"operation\": None, \"object_id\": 6, \"multiple\": True, \"pointing\": False},\n",
    "    87: {\"operation\": None, \"object_id\": 4, \"multiple\": True, \"pointing\": False},\n",
    "    66: {\"operation\": None, \"object_id\": 0, \"multiple\": True, \"pointing\": False},\n",
    "    73: {\"operation\": None, \"object_id\": 3, \"multiple\": True, \"pointing\": False},\n",
    "    83: {\"operation\": None, \"object_id\": 7, \"multiple\": True, \"pointing\": False},\n",
    "    88: {\"operation\": None, \"object_id\": 9, \"multiple\": True, \"pointing\": False},\n",
    "    78: {\"operation\": None, \"object_id\": 8, \"multiple\": True, \"pointing\": False},\n",
    "    71: {\"operation\": None, \"object_id\": 5, \"multiple\": True, \"pointing\": False},\n",
    "    65: {\"operation\": None, \"object_id\": 2, \"multiple\": True, \"pointing\": False},\n",
    "    10: {\"operation\": None, \"object_id\": 6, \"multiple\": False, \"pointing\": False},\n",
    "    19: {\"operation\": None, \"object_id\": 4, \"multiple\": False, \"pointing\": False},\n",
    "    11: {\"operation\": None, \"object_id\": 0, \"multiple\": False, \"pointing\": False},\n",
    "    18: {\"operation\": None, \"object_id\": 3, \"multiple\": False, \"pointing\": False},\n",
    "    21: {\"operation\": None, \"object_id\": 7, \"multiple\": False, \"pointing\": False},\n",
    "    20: {\"operation\": None, \"object_id\": 9, \"multiple\": False, \"pointing\": False},\n",
    "    14: {\"operation\": None, \"object_id\": 8, \"multiple\": False, \"pointing\": False},\n",
    "    13: {\"operation\": None, \"object_id\": 5, \"multiple\": False, \"pointing\": False},\n",
    "    16: {\"operation\": None, \"object_id\": 2, \"multiple\": False, \"pointing\": False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dictionary = {}\n",
    "\n",
    "for key in object_dictionary.keys():\n",
    "    new_key = tokenizer.sequences_to_texts([[key]])[0]\n",
    "    name_dictionary[new_key] = object_dictionary[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'laptops': {'operation': None,\n",
       "  'object_id': 6,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'phones': {'operation': None,\n",
       "  'object_id': 4,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'books': {'operation': None,\n",
       "  'object_id': 0,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'bottles': {'operation': None,\n",
       "  'object_id': 3,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'pens': {'operation': None,\n",
       "  'object_id': 7,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'cups': {'operation': None,\n",
       "  'object_id': 9,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'keyboards': {'operation': None,\n",
       "  'object_id': 8,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'mouses': {'operation': None,\n",
       "  'object_id': 5,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'monitors': {'operation': None,\n",
       "  'object_id': 2,\n",
       "  'multiple': True,\n",
       "  'pointing': False},\n",
       " 'laptop': {'operation': None,\n",
       "  'object_id': 6,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'phone': {'operation': None,\n",
       "  'object_id': 4,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'book': {'operation': None,\n",
       "  'object_id': 0,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'bottle': {'operation': None,\n",
       "  'object_id': 3,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'pen': {'operation': None,\n",
       "  'object_id': 7,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'cup': {'operation': None,\n",
       "  'object_id': 9,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'keyboard': {'operation': None,\n",
       "  'object_id': 8,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'mouse': {'operation': None,\n",
       "  'object_id': 5,\n",
       "  'multiple': False,\n",
       "  'pointing': False},\n",
       " 'monitor': {'operation': None,\n",
       "  'object_id': 2,\n",
       "  'multiple': False,\n",
       "  'pointing': False}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['laptops phones books bottles pens cups keyboards mouses monitors laptop phone book bottle pen cup keyboard mouse monitor']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = svm.SVC(kernel='rbf', gamma='scale')\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7721280602636534\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_train)\n",
    "print(accuracy_score(Y_train, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7471910112359551\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "print(accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1tgQvAm6prk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 164 unique tokens.\n",
      "Shape of data tensor: (709, 250)\n",
      "WARNING:tensorflow:From /home/darshanakg/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/darshanakg/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/darshanakg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 624 samples, validate on 70 samples\n",
      "WARNING:tensorflow:From /home/darshanakg/.conda/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/150\n",
      "624/624 [==============================] - 3s 5ms/sample - loss: 3.9172 - acc: 0.3750 - val_loss: 3.6302 - val_acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 3.3093 - acc: 0.3926 - val_loss: 3.1254 - val_acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 2.7932 - acc: 0.4535 - val_loss: 2.6747 - val_acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 2.3494 - acc: 0.5449 - val_loss: 2.3067 - val_acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 1.9515 - acc: 0.6058 - val_loss: 1.8900 - val_acc: 0.3857\n",
      "Epoch 6/150\n",
      "624/624 [==============================] - 2s 4ms/sample - loss: 1.5758 - acc: 0.8558 - val_loss: 1.8188 - val_acc: 0.1571\n",
      "Epoch 7/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 1.3124 - acc: 0.7901 - val_loss: 1.3593 - val_acc: 0.8143\n",
      "Epoch 8/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 1.0582 - acc: 0.9215 - val_loss: 1.1794 - val_acc: 0.8429\n",
      "Epoch 9/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.8634 - acc: 0.9359 - val_loss: 1.1991 - val_acc: 0.5857\n",
      "Epoch 10/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.7068 - acc: 0.9247 - val_loss: 1.0746 - val_acc: 0.6286\n",
      "Epoch 11/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.5998 - acc: 0.9391 - val_loss: 0.8313 - val_acc: 0.8000\n",
      "Epoch 12/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.4958 - acc: 0.9631 - val_loss: 0.8909 - val_acc: 0.7429\n",
      "Epoch 13/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.4216 - acc: 0.9567 - val_loss: 0.7667 - val_acc: 0.8000\n",
      "Epoch 14/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.3694 - acc: 0.9535 - val_loss: 0.7011 - val_acc: 0.8143\n",
      "Epoch 15/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.3171 - acc: 0.9696 - val_loss: 0.6258 - val_acc: 0.8143\n",
      "Epoch 16/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.2794 - acc: 0.9712 - val_loss: 0.7028 - val_acc: 0.8000\n",
      "Epoch 17/150\n",
      "624/624 [==============================] - 2s 4ms/sample - loss: 0.2491 - acc: 0.9712 - val_loss: 0.5056 - val_acc: 0.8286\n",
      "Epoch 18/150\n",
      "624/624 [==============================] - 2s 3ms/sample - loss: 0.2221 - acc: 0.9728 - val_loss: 0.7603 - val_acc: 0.7429\n",
      "Epoch 19/150\n",
      "624/624 [==============================] - 2s 4ms/sample - loss: 0.2052 - acc: 0.9712 - val_loss: 0.5269 - val_acc: 0.8143\n",
      "Epoch 20/150\n",
      "624/624 [==============================] - 2s 4ms/sample - loss: 0.2093 - acc: 0.9696 - val_loss: 0.6808 - val_acc: 0.7857\n",
      "178/178 [==============================] - 1s 3ms/sample - loss: 0.1716 - acc: 0.9831\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 250, 250)          12500000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 250, 250)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               140400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 12,640,703\n",
      "Trainable params: 12,640,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Test set\n",
      "  Loss: 0.172\n",
      "  Accuracy: 0.983\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df, sentences, y = import_and_prepare('data/dataset.txt')\n",
    "nltk.download('punkt')\n",
    "\n",
    "df, tokenizer = pre_initialize()\n",
    "model, history = lstm_train(df, tokenizer, MAX_NB_WORDS, MAX_SEQUENCE_LENGTH)\n",
    "model.save('lstm.h5')\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUYDRHX26voC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted vector:  [[0.9701294  0.00708048 0.02279018]]  Predicted Class:  Locate\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./lstm.h5')\n",
    "new_command = ['Track the pen']\n",
    "filtered_commands = filter_stopwords(new_command, stopwords_list)\n",
    "seq = tokenizer.texts_to_sequences(filtered_commands)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "\n",
    "labels = ['Locate', 'Describe', 'No_Op']\n",
    "print(\"Predicted vector: \", pred, \" Predicted Class: \", labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-i_QcHtHqG7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classifier",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
